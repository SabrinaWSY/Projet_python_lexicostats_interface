{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38028239b54b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyhanlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHanLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "\n",
    "result = HanLP.segment(data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segment3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3ba4ee4b0a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult_corpus1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_corpus1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'segment3' is not defined"
     ]
    }
   ],
   "source": [
    "with open('./corpus(sansEspace/51', 'r') as f:\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "\n",
    "result_corpus1 = segment3.seg(data)\n",
    "print(result_corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天/t 开心/a 了/y 吗/y ？/w\n"
     ]
    }
   ],
   "source": [
    "analyzer=PerceptronLexicalAnalyzer()\n",
    "a =analyzer.analyze(\"今天开心了吗？\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[中国科学院计算技术研究所/nt, 的/u, 宗/q, 成庆/vn, 教授/n, 正在/d, 教授/v, 自然语言处理/nz, 课程/n]\n"
     ]
    }
   ],
   "source": [
    "#REN\n",
    "NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')\n",
    "print(NLPTokenizer.segment('中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[中国科学院计算技术研究所/nt, 的/ude1, 宗成庆/nr, 教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 课程/n]\n"
     ]
    }
   ],
   "source": [
    "print(HanLP.segment('中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[译智社/n, 的/u, 田丰/nr, 要/v, 说/v, 的/u, 是/v, 这/r, 只/d, 是/v, 一个/m, hanlp命名/vn, 实体/n, 识别/v, 的/u, 例子/n]\n",
      "\n",
      "========== 命名实体开启与关闭对比试验 ==========\n",
      "\n",
      "crf :  [北川景子/nrj, 参演/v, 了/u, 林诣彬/nr, 导演/n, 的/u, 《/w, 速度/n, 与/c, 激情/n, 3/m, 》/w]\n",
      "crf_new :  [北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/mq, 》/w]\n",
      "viterbi :  [北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/m, 》/w]\n",
      "crf_new2 :  [北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/m, 》/w]\n",
      "crf :  [林志玲/nr, 亮相/v, 网友/n, :/w, 确定/v, 不/d, 是/v, 波多野/n, 结衣/n, ？/w]\n",
      "crf_new :  [林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不/d, 是/vshi, 波多野/n, 结衣/nz, ？/w]\n",
      "viterbi :  [林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不是/c, 波多野结衣/nrj, ？/w]\n",
      "crf_new2 :  [林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不是/c, 波多野结衣/nrj, ？/w]\n",
      "crf :  [龟/v, 山/n, 千/m, 广/q, 和/c, 近藤/a, 公园/n, 在/p, 龟山公园/ns, 里/f, 喝/v, 酒/n, 赏/v, 花/n]\n",
      "crf_new :  [龟/ng, 山/n, 千/mq, 广/a, 和/cc, 近藤/nz, 公园/n, 在/p, 龟山公园/n, 里/f, 喝/vg, 酒/nf, 赏/v, 花/n]\n",
      "viterbi :  [龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz]\n",
      "crf_new2 :  [龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz]\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "\"\"\"\n",
    "HanLP开启命名实体识别\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 音译人名示例\n",
    "CRFnewSegment = HanLP.newSegment(\"crf\")\n",
    "term_list = CRFnewSegment.seg(\"译智社的田丰要说的是这只是一个hanlp命名实体识别的例子\")\n",
    "print(term_list)\n",
    "\n",
    "\n",
    "print(\"\\n========== 命名实体开启与关闭对比试验 ==========\\n\")\n",
    "sentences =[\n",
    "    \"北川景子参演了林诣彬导演的《速度与激情3》\",\n",
    "    \"林志玲亮相网友:确定不是波多野结衣？\",\n",
    "    \"龟山千广和近藤公园在龟山公园里喝酒赏花\",\n",
    "]\n",
    "# 通过HanLP 进行全局设置,但是部分分词器本身可能不支持某项功能\n",
    "# 部分分词器本身对某些命名实体识别效果较好\n",
    "\n",
    "#HanLP.Config.PersonRecognition = False\n",
    "\n",
    "viterbiNewSegment = HanLP.newSegment(\"viterbi\").enableJapaneseNameRecognize(True)\n",
    "CRFnewSegment_new = HanLP.newSegment(\"crf\").enableJapaneseNameRecognize(True)\n",
    "CRFnewSegment_new2 = HanLP.newSegment().enableJapaneseNameRecognize(True)\n",
    "\n",
    "# segSentence\n",
    "# CRFnewSegment_2.seg2sentence(sentences)\n",
    "for sentence in sentences:\n",
    "    print(\"crf : \",CRFnewSegment.seg(sentence))\n",
    "    print(\"crf_new : \",CRFnewSegment_new.seg(sentence))\n",
    "    print(\"viterbi : \",viterbiNewSegment.seg(sentence))\n",
    "    print(\"crf_new2 : \",CRFnewSegment_new2.seg(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[我/rr, 在/p, 上海/ns, 林原科技有限公司/nt, 兼职/vn, 工作/vn]\n",
      "[我/rr, 经常/d, 在/p, 泰川喜宴餐厅/nt, 吃饭/vi]\n",
      "[偶尔/d, 去/vf, 开元地中海影城/nt, 看/v, 电影/n]\n",
      "\n",
      " =============机构名 标准分瓷器已经全部关闭=============\n",
      "\n",
      "[我/r, 在/p, 上海林原科技有限公司/nt, 兼职/vn, 工作/vn]\n",
      "[我/r, 经常/d, 在/p, 泰川/ns, 喜宴/n, 餐厅/n, 吃饭/v]\n",
      "[偶尔/d, 去/v, 开元/v, 地中海/ns, 影城/n, 看/v, 电影/n]\n",
      "\n",
      " =============segment2 Segmentation CRF, reconnaissance d'organisation open =============\n",
      "\n",
      "[我/r, 在/p, 上海林原科技有限公司/nt, 兼职/vn, 工作/vn]\n",
      "[我/r, 经常/d, 在/p, 泰川/ns, 喜宴/n, 餐厅/n, 吃饭/v]\n",
      "[偶尔/d, 去/v, 开元/v, 地中海/ns, 影城/n, 看/v, 电影/n]\n",
      "\n",
      " =============segment3 non CRF, reconnaissance d'organisation open =============\n",
      "\n",
      "[我/rr, 在/p, 上海/ns, 林原科技有限公司/nt, 兼职/vn, 工作/vn]\n",
      "[我/rr, 经常/d, 在/p, 泰川喜宴餐厅/nt, 吃饭/vi]\n",
      "[偶尔/d, 去/vf, 开元地中海影城/nt, 看/v, 电影/n]\n",
      "\n",
      " =============segment4 non CRF, reconnaissance d'organisation close =============\n",
      "\n",
      "[我/rr, 在/p, 上海/ns, 林原/nr, 科技/n, 有限公司/nis, 兼职/vn, 工作/vn]\n",
      "[我/rr, 经常/d, 在/p, 泰川/nr, 喜宴/n, 餐厅/nis, 吃饭/vi]\n",
      "[偶尔/d, 去/vf, 开元/nz, 地中海/nsf, 影城/n, 看/v, 电影/n]\n",
      "\n",
      " =============segment5 non CRF, par défaut =============\n",
      "\n",
      "[我/rr, 在/p, 上海/ns, 林原/nr, 科技/n, 有限公司/nis, 兼职/vn, 工作/vn]\n",
      "[我/rr, 经常/d, 在/p, 泰川/nr, 喜宴/n, 餐厅/nis, 吃饭/vi]\n",
      "[偶尔/d, 去/vf, 开元/nz, 地中海/nsf, 影城/n, 看/v, 电影/n]\n",
      "\n",
      " =============segment6 viterbi, reconnaissance d'organisation open =============\n",
      "\n",
      "[我/rr, 在/p, 上海/ns, 林原科技有限公司/nt, 兼职/vn, 工作/vn]\n",
      "[我/rr, 经常/d, 在/p, 泰川喜宴餐厅/nt, 吃饭/vi]\n",
      "[偶尔/d, 去/vf, 开元地中海影城/nt, 看/v, 电影/n]\n"
     ]
    }
   ],
   "source": [
    "#机构名识别\n",
    "sentences = [\n",
    "    \"我在上海林原科技有限公司兼职工作\",\n",
    "    \"我经常在泰川喜宴餐厅吃饭\",\n",
    "    \"偶尔去开元地中海影城看电影\",\n",
    "]\n",
    "\n",
    "Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "\n",
    "segment = HanLP.newSegment().enableOrganizationRecognize(True)\n",
    "for sentence in sentences:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)\n",
    "    \n",
    "print(\"\\n =============机构名 标准分瓷器已经全部关闭=============\\n\")\n",
    "print(CRFnewSegment.seg(sentences[0]))\n",
    "print(CRFnewSegment.seg(sentences[1]))\n",
    "print(CRFnewSegment.seg(sentences[2]))\n",
    "\n",
    "segment2 = HanLP.newSegment('crf').enableOrganizationRecognize(True)\n",
    "print(\"\\n =============segment2 Segmentation CRF, reconnaissance d'organisation open =============\\n\")\n",
    "print(segment2.seg(sentences[0]))\n",
    "print(segment2.seg(sentences[1]))\n",
    "print(segment2.seg(sentences[2]))\n",
    "\n",
    "segment3 = HanLP.newSegment().enableOrganizationRecognize(True)\n",
    "print(\"\\n =============segment3 non CRF, reconnaissance d'organisation open =============\\n\")\n",
    "print(segment3.seg(sentences[0]))\n",
    "print(segment3.seg(sentences[1]))\n",
    "print(segment3.seg(sentences[2]))\n",
    "\n",
    "segment4 = HanLP.newSegment().enableOrganizationRecognize(False)\n",
    "print(\"\\n =============segment4 non CRF, reconnaissance d'organisation close =============\\n\")\n",
    "print(segment4.seg(sentences[0]))\n",
    "print(segment4.seg(sentences[1]))\n",
    "print(segment4.seg(sentences[2]))\n",
    "\n",
    "segment5 = HanLP.newSegment()\n",
    "print(\"\\n =============segment5 non CRF, par défaut =============\\n\")\n",
    "print(segment5.seg(sentences[0]))\n",
    "print(segment5.seg(sentences[1]))\n",
    "print(segment5.seg(sentences[2]))\n",
    "\n",
    "segment6 = HanLP.newSegment('viterbi').enableOrganizationRecognize(True)\n",
    "print(\"\\n =============segment6 viterbi, reconnaissance d'organisation open =============\\n\")\n",
    "print(segment6.seg(sentences[0]))\n",
    "print(segment6.seg(sentences[1]))\n",
    "print(segment6.seg(sentences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[签约/vi, 仪式/n, 前/f, ，/w, 秦光荣/nr, 、/w, 李纪恒/nr, 、/w, 仇和/nr, 等/udeng, 一同/d, 会见/v, 了/ule, 参加/v, 签约/vi, 的/ude1, 企业家/nnt, 。/w]\n",
      "[武大靖/nr, 创/v, 世界纪录/nz, 夺冠/vi, ，/w, 中国代表团/nt, 平昌/ns, 首金/n]\n",
      "[区长/nnt, 庄木弟/nr, 新年/t, 致辞/vi]\n",
      "[朱立伦/nr, ：/w, 两岸/n, 都/d, 希望/v, 共创/v, 双赢/n,  /w, 习/v, 朱/ag, 历史/n, 会晤/vn, 在即/vi]\n",
      "[陕西/ns, 首富/n, 吴一坚/nr, 被/pbei, 带走/v,  /w, 与/cc, 令计划/nr, 妻子/n, 有/vyou, 交集/v]\n",
      "[据/p, 美国之音/n, 电台/nis, 网站/n, 4月/t, 28/m, 日/b, 报道/v, ，/w, 8/m, 岁/qt, 的/ude1, 凯瑟琳·克罗尔/nrf, （/w, 凤甫娟/nr, ）/w, 和/cc, 很多/m, 华裔/n, 美国/nsf, 小朋友/n, 一样/uyy, ，/w, 小小年纪/n, 就/d, 开始/v, 学/v, 小提琴/n, 了/ule, 。/w, 她/rr, 的/ude1, 妈妈/n, 是/vshi, 位/q, 虎妈/nz, 么/y, ？/w]\n",
      "[凯瑟琳/nrf, 和/cc, 露西/nrf, （/w, 庐瑞媛/nr, ）/w, ，/w, 跟/p, 她们/rr, 的/ude1, 哥哥/n, 们/k, 有/vyou, 一些/m, 不同/a, 。/w]\n",
      "[王国强/nr, 、/w, 高峰/n, 、/w, 汪洋/n, 、/w, 张朝阳/nr, 光着头/l, 、/w, 韩寒/nr, 、/w, 小/a, 四/m]\n",
      "[张浩/nr, 和/cc, 胡健康/nr, 复员/v, 回家/vi, 了/ule]\n",
      "[王总/nr, 和/cc, 小丽/nr, 结婚/vi, 了/ule]\n",
      "[编剧/nnt, 邵钧林/nr, 和/cc, 稽道青/nr, 说/v]\n",
      "[这里/rzs, 有/vyou, 关天培/nr, 的/ude1, 有关/vn, 事迹/n]\n",
      "[龚学平/nr, 等/udeng, 领导/n, 说/v, ,/w, 邓颖超/nr, 生前/t, 杜绝/v, 超生/vi]\n",
      "\n",
      "========== 中文人名 基本默认已开启 ==========\n",
      "\n",
      "[签约/vn, 仪式/n, 前/f, ，/w, 秦光荣/nr, 、/w, 李纪恒/nr, 、/w, 仇和/nr, 等/u, 一同/d, 会见/v, 了/u, 参加/v, 签约/v, 的/u, 企业家/n, 。/w]\n"
     ]
    }
   ],
   "source": [
    "# 中文人名识别\n",
    "def demo_chinese_name_recognition(sentences):\n",
    "    segment = HanLP.newSegment().enableNameRecognize(True);\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "        #print([i.word for i in term_list])\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"签约仪式前，秦光荣、李纪恒、仇和等一同会见了参加签约的企业家。\",\n",
    "    \"武大靖创世界纪录夺冠，中国代表团平昌首金\",\n",
    "    \"区长庄木弟新年致辞\",\n",
    "    \"朱立伦：两岸都希望共创双赢 习朱历史会晤在即\",\n",
    "    \"陕西首富吴一坚被带走 与令计划妻子有交集\",\n",
    "    \"据美国之音电台网站4月28日报道，8岁的凯瑟琳·克罗尔（凤甫娟）和很多华裔美国小朋友一样，小小年纪就开始学小提琴了。她的妈妈是位虎妈么？\",\n",
    "    \"凯瑟琳和露西（庐瑞媛），跟她们的哥哥们有一些不同。\",\n",
    "    \"王国强、高峰、汪洋、张朝阳光着头、韩寒、小四\",\n",
    "    \"张浩和胡健康复员回家了\",\n",
    "    \"王总和小丽结婚了\",\n",
    "    \"编剧邵钧林和稽道青说\",\n",
    "    \"这里有关天培的有关事迹\",\n",
    "    \"龚学平等领导说,邓颖超生前杜绝超生\",]\n",
    "demo_chinese_name_recognition(sentences)\n",
    "\n",
    "print(\"\\n========== 中文人名 基本默认已开启 ==========\\n\")\n",
    "print(CRFnewSegment.seg(sentences[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['布林线/nr', '熊也会/nr', '任泽平/nr', '高送/nr', '高送/nr', '陈果/nr', '鲁政委/nr', '黄金坑/nr', '云蒙/nr', '勾头/nr', '沃伦/nrf', '巴菲特/nrf', '钱为/nr', '周线和/nr', '周线/nr', '徐彪/nr', '徐彪/nr', '徐彪/nr', '刘名斌/nr', '姚卫巍/nr', '任泽平/nr', '任泽平/nr', '荀玉根/nr', '才出/nr', '那得/nr', '任泽平/nr', '波比/nrf', '任泽平/nr', '那波/nr', '阳包/nr', '安邦/nrf', '强拉/nrf', '强哥/nr', '强哥/nr', '强哥/nr', '小散/nr']\n",
      "36\n",
      "=================================\n",
      "['华尔街/nsf', '中国/ns', '贵州/ns', '中国/ns', '中国/ns', '贵州/ns', '贵州/ns', '中国/ns', '凤凰/ns', '中国/ns', '香港/ns', '广东/ns', '天津/ns', '福建/ns', '中国/ns', '中国/ns']\n",
      "16\n",
      "=================================\n",
      "['海通证券/nt', '中国银行/ntcb', '观市/nt', '中信证券/ntc', '中信证券/ntc', '家银行/nt', '方正证券/nt', '广发证券/nt', '全部华泰证券/nt', '浦发银行/ntcb', '中信证券/ntc', '格力电器/nt', '华泰证券/nt', '光大证券/ntc', '兴业银行/ntcb', '指数有/nt', '农业银行/ntcb', '26日上市机构/nt', '国泰/ntc', '国泰/ntc', '兴业银行/ntcb', '沪深大/nt', '中信证券/ntc', '中信证券/ntc', '招商银行/ntcb', '中信银行/ntcb', '中国石油/ntc', '中国石化/nto', '中国石油/ntc', '中国石化/nto', '中国石油/ntc', '上海银行/ntcb', '中国石化/nto', '北京银行/ntcb', '证券行业/nt', '西部证券/nt', '中信证券/ntc', '中国铝业/ntc', '中信证券/ntc', '华泰证券研究所/nt', '华泰/ntc', '证券研究所/nt', '广发证券/nt', '将会/nt', '有万达影院/nt', '华泰证券/nt', '家庭资产证券/nt', '新浪/ntc', '新浪/ntc', '交通银行/ntcb', '中信证券/ntc', '中国联通/ntc', '但联通/nt', '中信证券/ntc', '中国银行/ntcb', '国务院/nt', '中国中铁/nt', '中铁二局/nto', '中信证券/ntc', '中信证券/ntc', '中国建筑有/nt', '每个人/nt', '中国联通/ntc', '中信证券/ntc']\n",
      "64\n",
      "=================================\n",
      "['独狼/nz', '博文/nz', '做错/nz', '金地/nz', '满仓/nz', '中石油/nz', '中石油/nz', '涨停/nz', '自选股/nz', '创业板/nz', '创业板/nz', '股王/nz', '博文/nz', '杀跌/nz', '中石油/nz', '万科/nz', '寒气逼人/nz', '盲目乐观/nz', '国泰君安/nz', '京东方/nz', '炒楼/nz', '原始股/nz', '小昭/nz', '持续时间/nz', '三种人/nz', '大牛市/nz', '博文/nz', '广发/nz', '有可能/nz', '大牛市/nz', '列子/nz', '康威/nz', '大华/nz', '砸盘/nz', '第一桶金/nz', '大牛/nz', '主升浪/nz', '市盈率/nz', '偷笑/nz', '偷笑/nz', '怎么会/nz', '国泰君安/nz', '做空/nz', '博文/nz', '复权/nz', '国泰君安/nz', '博文/nz', '市盈率/nz', '最重要/nz', '涨停/nz', '涨停/nz', '涨停/nz', '涨停/nz', '一段时间/nz', '国泰君安/nz', '可能会/nz', '博文/nz', '放量/nz', '市盈率/nz', '市净率/nz', '国泰君安/nz', '上证指数/nz', '万科/nz', '大盘股/nz', '国泰君安/nz', '募资/nz', '放量/nz', '小阳线/nz', '可能会/nz', '次新股/nz', '国电电力/nz', '京东方/nz', '利空/nz', '利空/nz', '博文/nz', '二次元/nz', '二次元/nz', '企稳/nz', '国泰君安/nz', '财政支出/nz', '汇丰/nz', '深港/nz', '深港/nz', '短期内/nz', '市场走势/nz', '深港/nz', '把火/nz', '高开低走/nz', '减仓/nz', '打新股/nz', '一定会/nz', '创业板/nz', '海通/nz', '理财师/nz', '博文/nz', '不稳/nz', '持币观望/nz', '触底/nz', '营收/nz', '中国移动/nz', '中国移动/nz', '营收/nz', '饮鸩止渴/nz', '挤干/nz', '触底/nz', '爆仓/nz', '上证指数/nz', '大牛市/nz', '国泰君安/nz', '企稳/nz', '加息/nz', '巨量/nz', '国泰君安/nz', '博文/nz', '自贸区/nz', '套现/nz', '博文/nz', '大板块/nz', '消息面/nz', '创业板/nz', '宏源证券/nz', '举牌/nz', '止损/nz', '南京港/nz', '昌九/nz', '江苏阳光/nz', '南京港/nz', '避过/nz', '发威/nz', '后果自负/nz', '中信/nz']\n",
      "131\n",
      "=================================\n",
      "['五粮液/nf', '茅台/nf', '猪/nf', '茅台/nf', '茅台/nf']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "with open('./corpus(sansEspace/1', 'r') as f:\n",
    "    data1 = f.read()\n",
    "    f.close()\n",
    "\n",
    "result_corpus = segment3.seg(data1)\n",
    "\n",
    "import re\n",
    "datepat_nr = re.compile(r'\\w+/nr\\w*')\n",
    "liste_nr = datepat_nr.findall(str(result_corpus))\n",
    "print(liste_nr)\n",
    "print(len(liste_nr))\n",
    "\n",
    "print(\"=================================\")\n",
    "datepat_ns = re.compile(r'\\w+/ns\\w*')\n",
    "liste_ns = datepat_ns.findall(str(result_corpus))\n",
    "print(liste_ns)\n",
    "print(len(liste_ns))\n",
    "\n",
    "print(\"=================================\")\n",
    "datepat_nt = re.compile(r'\\w+/nt\\w*')\n",
    "liste_nt = datepat_nt.findall(str(result_corpus))\n",
    "print(liste_nt)\n",
    "print(len(liste_nt))\n",
    "\n",
    "print(\"=================================\")\n",
    "datepat_nz = re.compile(r'\\w+/nz\\w*')\n",
    "liste_nz = datepat_nz.findall(str(result_corpus))\n",
    "print(liste_nz)\n",
    "print(len(liste_nz))\n",
    "\n",
    "print(\"=================================\")\n",
    "datepat_nf = re.compile(r'\\w+/nf\\w*')\n",
    "liste_nf = datepat_nf.findall(str(result_corpus))\n",
    "print(liste_nf)\n",
    "print(len(liste_nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crf_new :  [龟/ng, 山/n, 千/mq, 广/a, 和/cc, 近藤/nz, 公园/n, 在/p, 龟山公园/n, 里/f, 喝/vg, 酒/nf, 赏/v, 花/n]\n",
      "============================================\n",
      "viterbi :  [龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz]\n"
     ]
    }
   ],
   "source": [
    "sentence0 = \"龟山千广和近藤公园在龟山公园里喝酒赏花\"\n",
    "print(\"crf_new : \",CRFnewSegment_new.seg(sentence0))\n",
    "print(\"============================================\")\n",
    "print(\"viterbi : \",viterbiNewSegment.seg(sentence0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除标签结果，转化为xml文件\n",
    "\n",
    "#def creatXML(corpusOri,newName)\n",
    "#-----------------将结果字符串化-----------\n",
    "pathCorpusOri = 'corpus(sansEspace/'+ corpusOri\n",
    "with open('corpus(sansEspace/101', 'r') as f:\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "\n",
    "result_corpus1 = segment3.seg(data)\n",
    "str_result = str(result_corpus1)\n",
    "#----------------------------------------\n",
    "\n",
    "#-----------------初步标注所有需要标注的EN------------\n",
    "import re\n",
    "\n",
    "resultWithTag1 = re.sub(r\"(\\w*?)\\/ns\\w?,\", r\"<ENLieu>\\1</ENLieu>\",str_result)\n",
    "resultWithTag2 = re.sub(r\"(\\w*?)\\/nt\\w*,\", r\"<ENOrganisation>\\1</ENOrganisation>\",resultWithTag1)\n",
    "resultWithTag3 = re.sub(r\"(\\w*?)\\/nz\\w?,\", r\"<ENAutre>\\1</ENAutre>\",resultWithTag2)\n",
    "resultWithTag4 = re.sub(r\"(\\w*?)\\/nf,\", r\"<ENAliment>\\1</ENAliment>\",resultWithTag3)\n",
    "resultWithTag5 = re.sub(r\"(\\w*?)\\/nr\\w?,\", r\"<ENPersonne>\\1</ENPersonne>\",resultWithTag4)\n",
    "#--------------------------------------------------\n",
    "\n",
    "#----------------Eliminer les autres étiquetages-------------------------\n",
    "resultFinal1 = re.sub(r\"(\\w+)/\\w+,\",r\"\\1\",resultWithTag5)\n",
    "resultFinal2 = re.sub(r\"(\\W)/\\w+,\",r\"\\1\",resultFinal1)\n",
    "resultFinal3 = re.sub(r\" \",r\"\",resultFinal2)\n",
    "resultFinal3 = resultFinal3[1:-3]\n",
    "#--------------------------------------------------\n",
    "\n",
    "#---------创造xml文件-------------\n",
    "resultXML = \"<?xml version=\\\"1.0\\\"?>\\n\"+\"<text>\\n\"+resultFinal3+\"</text>\"\n",
    "str_resultXML = str(resultXML)\n",
    "with open('corpusXML/hanlp/101.txt', 'w') as a:\n",
    "    a.write(str_resultXML)\n",
    "#-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(resultXML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除标签结果，转化为xml文件\n",
    "def creatXML(corpusOri,newName):\n",
    "\n",
    "    #-----------------将结果字符串化-----------\n",
    "    pathCorpusOri = 'corpus(sansEspace/'+ corpusOri\n",
    "    with open(pathCorpusOri, 'r') as f:\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "\n",
    "    result_corpus1 = segment3.seg(data)\n",
    "    str_result = str(result_corpus1)\n",
    "    #----------------------------------------\n",
    "\n",
    "    #-----------------trouver les entités nommées et remplacer les étiquetages------------\n",
    "    import re\n",
    "\n",
    "    resultWithTag1 = re.sub(r\"(\\w*?)\\/ns\\w?,\", r\"<ENLieu>\\1</ENLieu>\",str_result)\n",
    "    resultWithTag2 = re.sub(r\"(\\w*?)\\/nt\\w*,\", r\"<ENOrganisation>\\1</ENOrganisation>\",resultWithTag1)\n",
    "    resultWithTag3 = re.sub(r\"(\\w*?)\\/nz\\w?,\", r\"<ENAutre>\\1</ENAutre>\",resultWithTag2)\n",
    "    resultWithTag4 = re.sub(r\"(\\w*?)\\/nf,\", r\"<ENAliment>\\1</ENAliment>\",resultWithTag3)\n",
    "    resultWithTag5 = re.sub(r\"(\\w*?)\\/nr\\w?,\", r\"<ENPersonne>\\1</ENPersonne>\",resultWithTag4)\n",
    "    #--------------------------------------------------\n",
    "\n",
    "    #----------------Eliminer les autres étiquetages-------------------------\n",
    "    resultFinal1 = re.sub(r\"(\\w+)/\\w+,\",r\"\\1\",resultWithTag5)\n",
    "    resultFinal2 = re.sub(r\"(\\W)/\\w+,\",r\"\\1\",resultFinal1)\n",
    "    resultFinal3 = re.sub(r\" \",r\"\",resultFinal2)\n",
    "    resultFinal3 = resultFinal3[1:-3]\n",
    "    #--------------------------------------------------\n",
    "\n",
    "    #---------créer le fichier xml-------------\n",
    "    resultXML = \"<?xml version=\\\"1.0\\\"?>\\n\"+\"<text>\\n\"+resultFinal3+\"</text>\"\n",
    "    str_resultXML = str(resultXML)\n",
    "    pathCorpusNew = 'corpusXML/hanlp/' + newName + '.xml'\n",
    "    with open(pathCorpusNew, 'w') as a:\n",
    "        a.write(str_resultXML)\n",
    "    #-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatXML('51','51')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[忍者神龟/nz,  /w, （/w, 幻影/n, 工作室/nis, 漫画/n, ）/w,  /w, 编辑/nnt,  /w, 讨论/v, \n",
      "/w, 《/w, 忍者神龟/nz, 》/w, 源自/v, 幻影/n, 工作室/nis, （/w, Mirage/nx,  /w, Studios/nx, ）/w, 在/p, 1984/m, 年/qt, 发行/v, 的/ude1, 美国/nsf, 漫画/n, ，/w,  /w, 其/rz, 由/p, 凯文·伊斯曼/nrf, （/w, Kevin/nx,  /w, Eastman/nx, ）/w, 及/cc, 彼德·拉特/nrf, （/w, Peter/nx,  /w, Laird/nx, ）/w, 于/p, 1984/m, 年/qt, 开始/v, 创作/vn, ，/w, 并/cc, 由/p, 幻影/n, 工作室/nis, （/w, Mirage/nx,  /w, Studios/nx, ）/w, 出版/v, 。/w, 后/f, 推出/v, 1987/m, 年/qt, 电视/n, 动画版/nz, 、/w, 2003/m, 年/qt, 电视/n, 动画版/nz, 以及/cc, 电影版/nz, 等/udeng, 。/w, \n",
      "/w, 剧情/n, ：/w, 在/p, 纽约/nsf, 市/n, 一/m, 条/q, 大街/n, 的/ude1, 地下管道/nz, 里/f, 住着/v, 四/m, 只/d, 功夫/n, 高强/a, 的/ude1, 忍者神龟/nz, 和/cc, 他们/rr, 的/ude1, 老师/nnt, 斯普林特/nrf, —/w, 一/m, 只/d, 来自/v, 日本/ns, 的/ude1, 超级/b, 大/a, 老鼠/n, ，/w, 为了/p, 保卫/v, 城市/n, 家园/n, ，/w, 他们/rr, 同/p, 狡猾/a, 的/ude1, 巴克斯特·斯多克曼/nrf, 博士/nnt, 、/w, 紫龙/nz, 帮/v, 的/ude1, 坏蛋/n, 以及/cc, 一/m, 伙/q, 神秘/a, 且/c, 训练有素/vl, 的/ude1, 忍者/nnd, （/w, 史莱德/nr, 的/ude1, 部下/n, ）/w, 展开/v, 殊死/d, 搏斗/vi, ……/w]\n"
     ]
    }
   ],
   "source": [
    "import pyhanlp\n",
    "with open(\"testch.txt\") as f:\n",
    "    a = f.read()\n",
    "result = HanLP.segment(a)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
